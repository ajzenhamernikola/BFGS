{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    x3 = x[2]\n",
    "    \n",
    "    # local minima = (0, 5, 0)\n",
    "    # no globam minima\n",
    "    return x1**2 + (x2-5)**2 + x3**2 + np.sin(x1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_f(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    x3 = x[2]\n",
    "    \n",
    "    return np.array([2*x1 + 2*np.sin(x1)*np.cos(x1), 2*(x2-5), 2*x3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    \n",
    "    # global minima = (2/3, -5/3)\n",
    "    return -(5 + 3*x1 - 4*x2 - x1**2 + x1*x2 - x2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_g(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    \n",
    "    return np.array([2*x1 - x2 - 3, -x1 + 2*x2 + 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    \n",
    "    # global minima = every (x,y) such that x^2+2y^2=4\n",
    "    return (4 - x1**2 - 2*x2**2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_h(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    \n",
    "    return np.array([-4*x1*(-x1**2 - 2*(x2**2) + 4), -8*x2*(-x1**2 -2*(x2**2) + 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = np.array([f, g, h])\n",
    "gradients = np.array([grad_f, grad_g, grad_h])\n",
    "num_of_args = np.array([3, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom(x, p, phi, phi_grad, alpha_lo, alpha_hi, c1, c2):\n",
    "    \n",
    "    while True:\n",
    "        alpha_j = (alpha_hi + alpha_lo)/2\n",
    "        \n",
    "        phi_alpha_j = phi(alpha_j)\n",
    "        \n",
    "        if (phi_alpha_j > phi(0) + c1*alpha_j*phi_grad(0)) or (phi_alpha_j >= phi(alpha_lo)):\n",
    "            alpha_hi = alpha_j\n",
    "        else:\n",
    "            phi_grad_alpha_j = phi_grad(alpha_j)\n",
    "            \n",
    "            if np.abs(phi_grad_alpha_j) <= -c2*phi_grad(0):\n",
    "                return alpha_j\n",
    "            \n",
    "            if phi_grad_alpha_j*(alpha_hi - alpha_lo) >= 0:\n",
    "                alpha_hi = alpha_lo\n",
    "            \n",
    "            alpha_lo = alpha_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search_wolfe(fun, grad, x, p, maxiter=100, c1=10**(-3), c2=0.9, alpha_1=1.0, alpha_max=10**6):\n",
    "    if alpha_1 >= alpha_max:\n",
    "        raise ValueError('Argument alpha_1 should be less than alpha_max')\n",
    "    \n",
    "    def phi(alpha):\n",
    "        return fun(x + alpha*p)\n",
    "    \n",
    "    def phi_grad(alpha):\n",
    "        return np.dot(grad(x + alpha*p).T, p)\n",
    "    \n",
    "    alpha_old = 0\n",
    "    alpha_new = alpha_1\n",
    "    \n",
    "    final_alpha = None\n",
    "    \n",
    "    for i in np.arange(1, maxiter+1):\n",
    "        phi_alpha = phi(alpha_new)\n",
    "        \n",
    "        if (i == 1 and phi_alpha > phi(0) + c1*alpha_new*phi_grad(0)) or (i > 1 and phi_alpha >= phi(alpha_old)):\n",
    "            final_alpha = zoom(x, p, phi, phi_grad, alpha_old, alpha_new, c1, c2)\n",
    "            break\n",
    "        \n",
    "        phi_grad_alpha = phi_grad(alpha_new)\n",
    "        \n",
    "        if np.abs(phi_grad_alpha) <= -c2 * phi_grad(0):\n",
    "            final_alpha = alpha_new\n",
    "            break\n",
    "        \n",
    "        if phi_grad_alpha >= 0:\n",
    "            final_alpha = zoom(x, p, phi, phi_grad, alpha_new, alpha_old, c1, c2)\n",
    "            break\n",
    "            \n",
    "        alpha_old = alpha_new\n",
    "        alpha_new = alpha_new + (alpha_max - alpha_new) * np.random.rand(1)\n",
    "        \n",
    "    if i == maxiter and final_alpha is None:\n",
    "        return None\n",
    "\n",
    "    return final_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFGS(fun, grad, x_start, eps, max_iterations=100):\n",
    "    n = len(x_start)\n",
    "    B_old = np.diag(np.ones(n))\n",
    "    x_old = x_start\n",
    "    \n",
    "    for i in np.arange(1, max_iterations+1):\n",
    "        p = -1*np.dot(B_old, grad(x_old))\n",
    "        \n",
    "        alpha = line_search_wolfe(fun, grad, x_old, p, maxiter=max_iterations)\n",
    "        \n",
    "        if alpha is None:\n",
    "            print('Wolfe line search did not converge')\n",
    "            return x_old, i\n",
    "        \n",
    "        x_new = x_old + alpha*p\n",
    "        s = x_new - x_old\n",
    "        y = grad(x_new) - grad(x_old)\n",
    "        \n",
    "        I = np.diag(np.ones(n))\n",
    "        rho = 1 / np.dot(y.T, s)\n",
    "        B_new = np.dot(np.dot(I - rho*np.dot(s, y.T), B_old), I - rho*np.dot(y, s.T)) + rho*np.dot(s, s.T)\n",
    "        \n",
    "        print('x_k = {0} converges to x_(k+1) = {1}'.format(x_old, x_new))\n",
    "        \n",
    "        B_dist = np.linalg.norm(grad(x_old) - grad(x_new))\n",
    "        if B_dist < eps:\n",
    "            break\n",
    "        else:\n",
    "            print('There is still {0} left for approximations to converge'.format(np.abs(B_dist-eps)), '\\n')\n",
    "        \n",
    "        x_old = x_new\n",
    "        B_old = B_new\n",
    "        \n",
    "    print('\\nFinal approximation of the minima is {0}.'.format(x_new))\n",
    "    if i != max_iterations:\n",
    "        print('Optimization process converged in {0} steps'.format(i))\n",
    "    else:\n",
    "        print('Optimization process did not converge')\n",
    "        \n",
    "    return x_new, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Minimizing function f with the starting point [-11 -32  54]\n",
      "\n",
      "x_k = [-11 -32  54] converges to x_(k+1) = [-4.42565465e-03  5.00000000e+00  0.00000000e+00]\n",
      "There is still 132.75001633269696 left for approximations to converge \n",
      "\n",
      "x_k = [-4.42565465e-03  5.00000000e+00  0.00000000e+00] converges to x_(k+1) = [1.10640198e-03 5.00331924e+00 3.31924375e-03]\n",
      "There is still 0.02303730947284094 left for approximations to converge \n",
      "\n",
      "x_k = [1.10640198e-03 5.00331924e+00 3.31924375e-03] converges to x_(k+1) = [-3.07927373e-03  4.99906442e+00 -9.35584501e-04]\n",
      "There is still 0.01961905047552223 left for approximations to converge \n",
      "\n",
      "x_k = [-3.07927373e-03  4.99906442e+00 -9.35584501e-04] converges to x_(k+1) = [-1.10892998e-03  5.00099395e+00  9.93954997e-04]\n",
      "There is still 0.008586473176069618 left for approximations to converge \n",
      "\n",
      "x_k = [-1.10892998e-03  5.00099395e+00  9.93954997e-04] converges to x_(k+1) = [-9.88938667e-04  5.00110140e+00  1.10140016e-03]\n",
      "\n",
      "Final approximation of the minima is [-9.88938667e-04  5.00110140e+00  1.10140016e-03].\n",
      "Optimization process converged in 5 steps\n",
      "\n",
      "Minimizing function g with the starting point [-2 86]\n",
      "\n",
      "x_k = [-2 86] converges to x_(k+1) = [44.5 -3. ]\n",
      "There is still 289.00462278267185 left for approximations to converge \n",
      "\n",
      "x_k = [44.5 -3. ] converges to x_(k+1) = [-7.53314685 12.71685315]\n",
      "There is still 145.99460898957287 left for approximations to converge \n",
      "\n",
      "x_k = [-7.53314685 12.71685315] converges to x_(k+1) = [ 5.55157956 -8.07342044]\n",
      "There is still 72.06499781496993 left for approximations to converge \n",
      "\n",
      "x_k = [ 5.55157956 -8.07342044] converges to x_(k+1) = [-1.70674256  1.60575744]\n",
      "There is still 35.969617862410125 left for approximations to converge \n",
      "\n",
      "x_k = [-1.70674256  1.60575744] converges to x_(k+1) = [ 1.66071957 -3.49553043]\n",
      "There is still 18.005716673973996 left for approximations to converge \n",
      "\n",
      "x_k = [ 1.66071957 -3.49553043] converges to x_(k+1) = [ 0.49150301 -0.43037199]\n",
      "There is still 9.08095960160922 left for approximations to converge \n",
      "\n",
      "x_k = [ 0.49150301 -0.43037199] converges to x_(k+1) = [ 0.14611574 -2.89294676]\n",
      "There is still 4.909549718625257 left for approximations to converge \n",
      "\n",
      "x_k = [ 0.14611574 -2.89294676] converges to x_(k+1) = [ 1.29026041 -1.21950521]\n",
      "There is still 2.285939962088711 left for approximations to converge \n",
      "\n",
      "x_k = [ 1.29026041 -1.21950521] converges to x_(k+1) = [ 0.70010315 -1.74350036]\n",
      "There is still 0.7992288504335079 left for approximations to converge \n",
      "\n",
      "x_k = [ 0.70010315 -1.74350036] converges to x_(k+1) = [ 0.7146815  -1.64621938]\n",
      "There is still 0.19144485765938105 left for approximations to converge \n",
      "\n",
      "x_k = [ 0.7146815  -1.64621938] converges to x_(k+1) = [ 0.66038239 -1.69018066]\n",
      "There is still 0.07185925836324927 left for approximations to converge \n",
      "\n",
      "x_k = [ 0.66038239 -1.69018066] converges to x_(k+1) = [ 0.68218081 -1.6619211 ]\n",
      "There is still 0.03695734357291643 left for approximations to converge \n",
      "\n",
      "x_k = [ 0.68218081 -1.6619211 ] converges to x_(k+1) = [ 0.6606925  -1.67937119]\n",
      "There is still 0.027835426661592288 left for approximations to converge \n",
      "\n",
      "x_k = [ 0.6606925  -1.67937119] converges to x_(k+1) = [ 0.67017955 -1.6686222 ]\n",
      "There is still 0.01355730103936403 left for approximations to converge \n",
      "\n",
      "x_k = [ 0.67017955 -1.6686222 ] converges to x_(k+1) = [ 0.66436814 -1.6703323 ]\n",
      "There is still 0.009197058481013012 left for approximations to converge \n",
      "\n",
      "x_k = [ 0.66436814 -1.6703323 ] converges to x_(k+1) = [ 0.66809618 -1.66634792]\n",
      "There is still 0.004480541529833258 left for approximations to converge \n",
      "\n",
      "x_k = [ 0.66809618 -1.66634792] converges to x_(k+1) = [ 0.66675266 -1.66748317]\n",
      "There is still 0.0008075799192918136 left for approximations to converge \n",
      "\n",
      "x_k = [ 0.66675266 -1.66748317] converges to x_(k+1) = [ 0.66723096 -1.66683566]\n",
      "\n",
      "Final approximation of the minima is [ 0.66723096 -1.66683566].\n",
      "Optimization process converged in 18 steps\n",
      "\n",
      "Minimizing function h with the starting point [ 98 -56]\n",
      "\n",
      "x_k = [ 98 -56] converges to x_(k+1) = [ 3.0625 52.5   ]\n",
      "There is still 11258989.5010582 left for approximations to converge \n",
      "\n",
      "x_k = [ 3.0625 52.5   ] converges to x_(k+1) = [  0.99875299 -18.22570758]\n",
      "There is still 2414811.436689629 left for approximations to converge \n",
      "\n",
      "x_k = [  0.99875299 -18.22570758] converges to x_(k+1) = [0.35467545 5.31740382]\n",
      "There is still 98702.78750159536 left for approximations to converge \n",
      "\n",
      "x_k = [0.35467545 5.31740382] converges to x_(k+1) = [ 0.06021868 -3.43814169]\n",
      "There is still 2781.994687933606 left for approximations to converge \n",
      "\n",
      "x_k = [ 0.06021868 -3.43814169] converges to x_(k+1) = [0.03760215 0.79766071]\n",
      "There is still 522.9740822453427 left for approximations to converge \n",
      "\n",
      "x_k = [0.03760215 0.79766071] converges to x_(k+1) = [0.05683079 1.34769405]\n",
      "There is still 13.471979228167093 left for approximations to converge \n",
      "\n",
      "x_k = [0.05683079 1.34769405] converges to x_(k+1) = [0.06140613 1.41233149]\n",
      "There is still 3.849014346151116 left for approximations to converge \n",
      "\n",
      "x_k = [0.06140613 1.41233149] converges to x_(k+1) = [0.06151827 1.4136299 ]\n",
      "There is still 0.08209578015004754 left for approximations to converge \n",
      "\n",
      "x_k = [0.06151827 1.4136299 ] converges to x_(k+1) = [0.061509   1.41353705]\n",
      "There is still 0.004951507875514205 left for approximations to converge \n",
      "\n",
      "x_k = [0.061509   1.41353705] converges to x_(k+1) = [0.06150994 1.41354536]\n",
      "\n",
      "Final approximation of the minima is [0.06150994 1.41354536].\n",
      "Optimization process converged in 10 steps\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(n):\n",
    "    x_start = np.random.randint(-100, 100, num_of_args[i])\n",
    "    print('\\nMinimizing function {0} with the starting point {1}\\n'.format(functions[i].__name__, x_start))\n",
    "    BFGS(functions[i], gradients[i], x_start, 0.001, max_iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
