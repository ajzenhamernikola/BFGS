{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line search algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finds the step $\\alpha^*$ used in the <code>BFGS</code> algorithm in the direction given by <code>p</code>. The step needs to satisfy the strong Wolfe conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search_wolfe(fun, grad, x, p, maxiter=100, c1=10**(-3), c2=0.9, alpha_1=1.0, alpha_max=10**6):\n",
    "    if alpha_1 >= alpha_max:\n",
    "        raise ValueError('Argument alpha_1 should be less than alpha_max')\n",
    "    \n",
    "    def phi(alpha):\n",
    "        return fun(x + alpha*p)\n",
    "    \n",
    "    def phi_grad(alpha):\n",
    "        return np.dot(grad(x + alpha*p).T, p)\n",
    "    \n",
    "    alpha_old = 0\n",
    "    alpha_new = alpha_1\n",
    "    \n",
    "    final_alpha = None\n",
    "    \n",
    "    for i in np.arange(1, maxiter+1):\n",
    "        phi_alpha = phi(alpha_new)\n",
    "        \n",
    "        if (i == 1 and phi_alpha > phi(0) + c1*alpha_new*phi_grad(0)) or (i > 1 and phi_alpha >= phi(alpha_old)):\n",
    "            final_alpha = zoom(x, p, phi, phi_grad, alpha_old, alpha_new, c1, c2)\n",
    "            break\n",
    "        \n",
    "        phi_grad_alpha = phi_grad(alpha_new)\n",
    "        \n",
    "        if np.abs(phi_grad_alpha) <= -c2 * phi_grad(0):\n",
    "            final_alpha = alpha_new\n",
    "            break\n",
    "        \n",
    "        if phi_grad_alpha >= 0:\n",
    "            final_alpha = zoom(x, p, phi, phi_grad, alpha_new, alpha_old, c1, c2)\n",
    "            break\n",
    "            \n",
    "        alpha_old = alpha_new\n",
    "        alpha_new = alpha_new + (alpha_max - alpha_new) * np.random.rand(1)\n",
    "        \n",
    "    if i == maxiter and final_alpha is None:\n",
    "        return None\n",
    "\n",
    "    return final_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliary function <code>zoom</code> searches for the final value <code>alpha_j</code> in the range \\[<code>alpha_lo</code>, <code>alpha_hi</code>\\] that will be used as the optimal step $\\alpha^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom(x, p, phi, phi_grad, alpha_lo, alpha_hi, c1, c2):\n",
    "    \n",
    "    while True:\n",
    "        alpha_j = (alpha_hi + alpha_lo)/2\n",
    "        \n",
    "        phi_alpha_j = phi(alpha_j)\n",
    "        \n",
    "        if (phi_alpha_j > phi(0) + c1*alpha_j*phi_grad(0)) or (phi_alpha_j >= phi(alpha_lo)):\n",
    "            alpha_hi = alpha_j\n",
    "        else:\n",
    "            phi_grad_alpha_j = phi_grad(alpha_j)\n",
    "            \n",
    "            if np.abs(phi_grad_alpha_j) <= -c2*phi_grad(0):\n",
    "                return alpha_j\n",
    "            \n",
    "            if phi_grad_alpha_j*(alpha_hi - alpha_lo) >= 0:\n",
    "                alpha_hi = alpha_lo\n",
    "            \n",
    "            alpha_lo = alpha_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFGS algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main implementation of the BFGS algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFGS(fun, grad, x_start, eps, max_iterations=100, verbose=False):\n",
    "    n = len(x_start)\n",
    "    \n",
    "    # We are starting with identity matrix \n",
    "    # as approximation of the inverse of the Hessian.\n",
    "    # It will be updated on every iteration.\n",
    "    # We are using the notation H_k = (B_k)^{-1},\n",
    "    # where B_k is the approximation of the Hessian.\n",
    "    H_old = np.diag(np.ones(n))\n",
    "    x_old = x_start\n",
    "    \n",
    "    for i in np.arange(1, max_iterations+1):\n",
    "        # Search direction\n",
    "        p = -1*np.dot(H_old, grad(x_old))\n",
    "        \n",
    "        # Calculating the step into the direction p\n",
    "        # using the Wolfe conditions as constrains on the step.\n",
    "        alpha = line_search_wolfe(fun, grad, x_old, p, maxiter=max_iterations)\n",
    "        \n",
    "        if alpha is None:\n",
    "            print('Wolfe line search did not converge')\n",
    "            return x_old, i\n",
    "        \n",
    "        x_new = x_old + alpha*p\n",
    "        \n",
    "        s = (x_new - x_old).reshape((n, 1))\n",
    "        y = (grad(x_new) - grad(x_old)).reshape((n, 1))\n",
    "        sT = s.T.reshape((1, n))\n",
    "        yT = y.T.reshape((1, n))\n",
    "        \n",
    "        yT_s = np.dot(yT, s).reshape(())\n",
    "        \n",
    "        I = np.diag(np.ones(n))\n",
    "        rho = 1 / yT_s\n",
    "        rho2 = rho**2\n",
    "        \n",
    "        # The next products are being used \n",
    "        # in the calculation of the H_{k+1} from H_k.\n",
    "        # Only the matrices of dimension (n x n) will be used in the final formula.\n",
    "        H_y         = np.dot(H_old,    y).reshape((n, 1)) # H_k * y_k\n",
    "        Hy_sT       = np.dot(H_y,     sT).reshape((n, n)) # (H_k*y_k) * s^T\n",
    "        yT_H        = np.dot(yT,   H_old).reshape((1, n)) # y_k^T * H_k\n",
    "        s_yTH       = np.dot(s,     yT_H).reshape((n, n)) # s_k * (y_k^T*H_k)\n",
    "        syTH_y      = np.dot(s_yTH,    y).reshape((n, 1)) # (s_k*(y_k^T*H_k)) * y_k\n",
    "        syTHy_sT    = np.dot(syTH_y,  sT).reshape((n, n)) # ((s_k*(y_k^T*H_k))*y_k) * s_k^T\n",
    "        s_sT        = np.dot(s,       sT).reshape((n, n)) # s_k * s_k^T\n",
    "        \n",
    "        # The initial formula \n",
    "        # H_{k+1} = (I - rho_k*s_k*y_k^T)H_k(I - rho_k*y_k*s_k^T) + rho_k*s_k*s_T\n",
    "        # can be rewritten as \n",
    "        # H_{k+1} = H_k - rho_k*(H_k*y_k)*s_k^T - rho_k*s_k*(y_k^T*H_k) + rho_k^2*((s_k*(y_k^T*H_k))*y_k)*s_k^T + rho_k*s_k*s_k^T\n",
    "        # to avoid calculations of assimptote complexity O(n^3).\n",
    "        H_new = H_old - rho*Hy_sT - rho*s_yTH + rho2*syTHy_sT + rho*s_sT\n",
    "        \n",
    "        if verbose:\n",
    "            print('x_k = {0} converges to x_(k+1) = {1}'.format(x_old, x_new))\n",
    "        \n",
    "        # We are using the 2-norm value \n",
    "        # between the previous and the next gradient\n",
    "        # of the approximation of the function minima\n",
    "        # as the stopping condition for the BFGS algorithm.\n",
    "        grad_dist = np.linalg.norm(grad(x_old) - grad(x_new))\n",
    "        if grad_dist < eps:\n",
    "            break\n",
    "        elif verbose:\n",
    "            print('There is still {0} left for approximations to converge'.format(np.abs(grad_dist-eps)), '\\n')\n",
    "        \n",
    "        x_old = x_new\n",
    "        H_old = H_new\n",
    "        \n",
    "    print('\\nFinal approximation of the minima is {0}.'.format(x_new))\n",
    "    if i != max_iterations:\n",
    "        print('Optimization process converged in {0} steps'.format(i))\n",
    "    else:\n",
    "        print('Optimization process did not converge')\n",
    "        \n",
    "    return x_new, i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    x3 = x[2]\n",
    "    \n",
    "    # local minima = (0, 5, 0)\n",
    "    # no globam minima\n",
    "    return x1**2 + (x2-5)**2 + x3**2 + np.sin(x1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_f(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    x3 = x[2]\n",
    "    \n",
    "    return np.array([2*x1 + 2*np.sin(x1)*np.cos(x1), 2*(x2-5), 2*x3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    \n",
    "    # global minima = (2/3, -5/3)\n",
    "    return -(5 + 3*x1 - 4*x2 - x1**2 + x1*x2 - x2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_g(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    \n",
    "    return np.array([2*x1 - x2 - 3, -x1 + 2*x2 + 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    \n",
    "    # global minima = every (x,y) such that x^2+2y^2=4\n",
    "    return (4 - x1**2 - 2*x2**2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_h(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    \n",
    "    return np.array([-4*x1*(-x1**2 - 2*(x2**2) + 4), -8*x2*(-x1**2 -2*(x2**2) + 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = np.array([f, g, h])\n",
    "gradients = np.array([grad_f, grad_g, grad_h])\n",
    "num_of_args = np.array([3, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Minimizing function f with the starting point [ 2 79 -8]\n",
      "\n",
      "\n",
      "Final approximation of the minima is [-2.28534530e-14  5.00000000e+00 -3.57913974e-15].\n",
      "Optimization process converged in 5 steps\n",
      "\n",
      "Built-in function results:\n",
      "Final approximation of the minima is [-4.41752949e-07  4.99999991e+00  9.83721514e-09]\n",
      "Optimization process converged in 9 steps\n",
      "\n",
      "Minimizing function g with the starting point [-86   6]\n",
      "\n",
      "\n",
      "Final approximation of the minima is [ 0.6666666  -1.66666538].\n",
      "Optimization process converged in 5 steps\n",
      "\n",
      "Built-in function results:\n",
      "Final approximation of the minima is [ 0.66666651 -1.66666668]\n",
      "Optimization process converged in 5 steps\n",
      "\n",
      "Minimizing function h with the starting point [-29  88]\n",
      "\n",
      "\n",
      "Final approximation of the minima is [-1.58842403  0.85933379].\n",
      "Optimization process converged in 22 steps\n",
      "\n",
      "Built-in function results:\n",
      "Final approximation of the minima is [-0.38677496  1.38751669]\n",
      "Optimization process converged in 24 steps\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(n):\n",
    "    x_start = np.random.randint(-100, 100, num_of_args[i])\n",
    "    print('\\nMinimizing function {0} with the starting point {1}\\n'.format(functions[i].__name__, x_start))\n",
    "    BFGS(functions[i], gradients[i], x_start, 0.001, max_iterations=30)\n",
    "    \n",
    "    result = optimize.minimize(functions[i], x_start, method='BFGS', jac=gradients[i])\n",
    "    print('\\nBuilt-in function results:')\n",
    "    print('Final approximation of the minima is', result['x'])\n",
    "    if result['success']:\n",
    "        print('Optimization process converged in {0} steps'.format(result['nit']))\n",
    "    else:\n",
    "        print('Optimization process did not converge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
