{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    x3 = x[2]\n",
    "    \n",
    "    # local minima = (0, 5, 0)\n",
    "    # no globam minima\n",
    "    return x1**2 + (x2-5)**2 + x3**2 + np.sin(x1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_f(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    x3 = x[2]\n",
    "    \n",
    "    return np.array([2*x1 + 2*np.sin(x1)*np.cos(x1), 2*(x2-5), 2*x3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    \n",
    "    # global minima = (2/3, -5/3)\n",
    "    return -(5 + 3*x1 - 4*x2 - x1**2 + x1*x2 - x2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_g(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    \n",
    "    return np.array([2*x1 - x2 - 3, -x1 + 2*x2 + 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    \n",
    "    # global minima = every (x,y) such that x^2+2y^2=4\n",
    "    return (4 - x1**2 - 2*x2**2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_h(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    \n",
    "    return np.array([-4*x1*(-x1**2 - 2*(x2**2) + 4), -8*x2*(-x1**2 -2*(x2**2) + 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = np.array([f, g, h])\n",
    "gradients = np.array([grad_f, grad_g, grad_h])\n",
    "num_of_args = np.array([3, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom(x, p, phi, phi_grad, alpha_lo, alpha_hi, c1, c2):\n",
    "    \n",
    "    while True:\n",
    "        alpha_j = (alpha_hi + alpha_lo)/2\n",
    "        \n",
    "        phi_alpha_j = phi(alpha_j)\n",
    "        \n",
    "        if (phi_alpha_j > phi(0) + c1*alpha_j*phi_grad(0)) or (phi_alpha_j >= phi(alpha_lo)):\n",
    "            alpha_hi = alpha_j\n",
    "        else:\n",
    "            phi_grad_alpha_j = phi_grad(alpha_j)\n",
    "            \n",
    "            if np.abs(phi_grad_alpha_j) <= -c2*phi_grad(0):\n",
    "                return alpha_j\n",
    "            \n",
    "            if phi_grad_alpha_j*(alpha_hi - alpha_lo) >= 0:\n",
    "                alpha_hi = alpha_lo\n",
    "            \n",
    "            alpha_lo = alpha_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search_wolfe(fun, grad, x, p, maxiter=100, c1=10**(-3), c2=0.9, alpha_1=1.0, alpha_max=10**6):\n",
    "    if alpha_1 >= alpha_max:\n",
    "        raise ValueError('Argument alpha_1 should be less than alpha_max')\n",
    "    \n",
    "    def phi(alpha):\n",
    "        return fun(x + alpha*p)\n",
    "    \n",
    "    def phi_grad(alpha):\n",
    "        return np.dot(grad(x + alpha*p).T, p)\n",
    "    \n",
    "    alpha_old = 0\n",
    "    alpha_new = alpha_1\n",
    "    \n",
    "    final_alpha = None\n",
    "    \n",
    "    for i in np.arange(1, maxiter+1):\n",
    "        phi_alpha = phi(alpha_new)\n",
    "        \n",
    "        if (i == 1 and phi_alpha > phi(0) + c1*alpha_new*phi_grad(0)) or (i > 1 and phi_alpha >= phi(alpha_old)):\n",
    "            final_alpha = zoom(x, p, phi, phi_grad, alpha_old, alpha_new, c1, c2)\n",
    "            break\n",
    "        \n",
    "        phi_grad_alpha = phi_grad(alpha_new)\n",
    "        \n",
    "        if np.abs(phi_grad_alpha) <= -c2 * phi_grad(0):\n",
    "            final_alpha = alpha_new\n",
    "            break\n",
    "        \n",
    "        if phi_grad_alpha >= 0:\n",
    "            final_alpha = zoom(x, p, phi, phi_grad, alpha_new, alpha_old, c1, c2)\n",
    "            break\n",
    "            \n",
    "        alpha_old = alpha_new\n",
    "        alpha_new = alpha_new + (alpha_max - alpha_new) * np.random.rand(1)\n",
    "        \n",
    "    if i == maxiter and final_alpha is None:\n",
    "        return None\n",
    "\n",
    "    return final_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFGS(fun, grad, x_start, eps, max_iterations=100):\n",
    "    n = len(x_start)\n",
    "    \n",
    "    # We are starting with identity matrix \n",
    "    # as approximation of the inverse of the Hessian.\n",
    "    # It will be updated on every iteration.\n",
    "    # We are using the notation H_k = (B_k)^{-1},\n",
    "    # where B_k is the approximation of the Hessian.\n",
    "    H_old = np.diag(np.ones(n))\n",
    "    x_old = x_start\n",
    "    \n",
    "    for i in np.arange(1, max_iterations+1):\n",
    "        # Search direction\n",
    "        p = -1*np.dot(H_old, grad(x_old))\n",
    "        \n",
    "        # Calculating the step into the direction p\n",
    "        # using the Wolfe conditions as constrains on the step.\n",
    "        alpha = line_search_wolfe(fun, grad, x_old, p, maxiter=max_iterations)\n",
    "        \n",
    "        if alpha is None:\n",
    "            print('Wolfe line search did not converge')\n",
    "            return x_old, i\n",
    "        \n",
    "        x_new = x_old + alpha*p\n",
    "        \n",
    "        s = (x_new - x_old).reshape((n, 1))\n",
    "        y = (grad(x_new) - grad(x_old)).reshape((n, 1))\n",
    "        sT = s.T.reshape((1, n))\n",
    "        yT = y.T.reshape((1, n))\n",
    "        \n",
    "        yT_s = np.dot(yT, s).reshape(())\n",
    "        \n",
    "        I = np.diag(np.ones(n))\n",
    "        rho = 1 / yT_s\n",
    "        rho2 = rho**2\n",
    "        \n",
    "        # The next products are being used \n",
    "        # in the calculation of the H_{k+1} from H_k.\n",
    "        # Only the matrices of dimension (n x n) will be used in the final formula.\n",
    "        H_y         = np.dot(H_old,    y).reshape((n, 1)) # H_k * y_k\n",
    "        Hy_sT       = np.dot(H_y,     sT).reshape((n, n)) # (H_k*y_k) * s^T\n",
    "        yT_H        = np.dot(yT,   H_old).reshape((1, n)) # y_k^T * H_k\n",
    "        s_yTH       = np.dot(s,     yT_H).reshape((n, n)) # s_k * (y_k^T*H_k)\n",
    "        syTH_y      = np.dot(s_yTH,    y).reshape((n, 1)) # (s_k*(y_k^T*H_k)) * y_k\n",
    "        syTHy_sT    = np.dot(syTH_y,  sT).reshape((n, n)) # ((s_k*(y_k^T*H_k))*y_k) * s_k^T\n",
    "        s_sT        = np.dot(s,       sT).reshape((n, n)) # s_k * s_k^T\n",
    "        \n",
    "        # The initial formula \n",
    "        # H_{k+1} = (I - rho_k*s_k*y_k^T)H_k(I - rho_k*y_k*s_k^T) + rho_k*s_k*s_T\n",
    "        # can be rewritten as \n",
    "        # H_{k+1} = H_k - rho_k*(H_k*y_k)*s_k^T - rho_k*s_k*(y_k^T*H_k) + rho_k^2*((s_k*(y_k^T*H_k))*y_k)*s_k^T + rho_k*s_k*s_k^T\n",
    "        # to avoid calculations of assimptote complexity O(n^3).\n",
    "        H_new = H_old - rho*Hy_sT - rho*s_yTH + rho2*syTHy_sT + rho*s_sT\n",
    "        \n",
    "        print('x_k = {0} converges to x_(k+1) = {1}'.format(x_old, x_new))\n",
    "        \n",
    "        # We are using the 2-norm value \n",
    "        # between the previous and the next gradient\n",
    "        # of the approximation of the function minima\n",
    "        # as the stopping condition for the BFGS algorithm.\n",
    "        grad_dist = np.linalg.norm(grad(x_old) - grad(x_new))\n",
    "        if grad_dist < eps:\n",
    "            break\n",
    "        else:\n",
    "            print('There is still {0} left for approximations to converge'.format(np.abs(grad_dist-eps)), '\\n')\n",
    "        \n",
    "        x_old = x_new\n",
    "        H_old = H_new\n",
    "        \n",
    "    print('\\nFinal approximation of the minima is {0}.'.format(x_new))\n",
    "    if i != max_iterations:\n",
    "        print('Optimization process converged in {0} steps'.format(i))\n",
    "    else:\n",
    "        print('Optimization process did not converge')\n",
    "        \n",
    "    return x_new, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Minimizing function f with the starting point [-80   2  21]\n",
      "\n",
      "x_k = [-80   2  21] converges to x_(k+1) = [0.10971263 5.         0.        ]\n",
      "There is still 166.16312719435476 left for approximations to converge \n",
      "\n",
      "x_k = [0.10971263 5.         0.        ] converges to x_(k+1) = [-0.00638865  5.00381564 -0.02670949]\n",
      "There is still 0.46478449869802085 left for approximations to converge \n",
      "\n",
      "x_k = [-0.00638865  5.00381564 -0.02670949] converges to x_(k+1) = [-2.37271554e-03  4.99709376e+00  2.03436751e-02]\n",
      "There is still 0.09540937678557701 left for approximations to converge \n",
      "\n",
      "x_k = [-2.37271554e-03  4.99709376e+00  2.03436751e-02] converges to x_(k+1) = [ 2.75644219e-04  5.00000659e+00 -4.61024943e-05]\n",
      "There is still 0.04153387949643447 left for approximations to converge \n",
      "\n",
      "x_k = [ 2.75644219e-04  5.00000659e+00 -4.61024943e-05] converges to x_(k+1) = [-1.62239964e-05  5.00000059e+00 -4.13014627e-06]\n",
      "There is still 0.00017054830539122985 left for approximations to converge \n",
      "\n",
      "x_k = [-1.62239964e-05  5.00000059e+00 -4.13014627e-06] converges to x_(k+1) = [2.26262201e-09 5.00000000e+00 3.08331127e-08]\n",
      "\n",
      "Final approximation of the minima is [2.26262201e-09 5.00000000e+00 3.08331127e-08].\n",
      "Optimization process converged in 6 steps\n",
      "\n",
      "Minimizing function g with the starting point [-26 -13]\n",
      "\n",
      "x_k = [-26 -13] converges to x_(k+1) = [ -5. -15.]\n",
      "There is still 50.60532371551998 left for approximations to converge \n",
      "\n",
      "x_k = [ -5. -15.] converges to x_(k+1) = [3.43819597 3.21122491]\n",
      "There is still 28.01507114270046 left for approximations to converge \n",
      "\n",
      "x_k = [3.43819597 3.21122491] converges to x_(k+1) = [ 0.66384941 -1.66680105]\n",
      "There is still 7.01284373977571 left for approximations to converge \n",
      "\n",
      "x_k = [ 0.66384941 -1.66680105] converges to x_(k+1) = [ 0.66668046 -1.66666799]\n",
      "There is still 0.005095032166650001 left for approximations to converge \n",
      "\n",
      "x_k = [ 0.66668046 -1.66666799] converges to x_(k+1) = [ 0.66666667 -1.66666667]\n",
      "\n",
      "Final approximation of the minima is [ 0.66666667 -1.66666667].\n",
      "Optimization process converged in 5 steps\n",
      "\n",
      "Minimizing function h with the starting point [16 -1]\n",
      "\n",
      "x_k = [16 -1] converges to x_(k+1) = [0.125    0.984375]\n",
      "There is still 16381.531109409789 left for approximations to converge \n",
      "\n",
      "x_k = [0.125    0.984375] converges to x_(k+1) = [0.18695602 1.48426081]\n",
      "There is still 21.39374404539113 left for approximations to converge \n",
      "\n",
      "x_k = [0.18695602 1.48426081] converges to x_(k+1) = [0.17176381 1.36166797]\n",
      "There is still 8.108104591094513 left for approximations to converge \n",
      "\n",
      "x_k = [0.17176381 1.36166797] converges to x_(k+1) = [0.17712608 1.4049372 ]\n",
      "There is still 2.6254039194784085 left for approximations to converge \n",
      "\n",
      "x_k = [0.17712608 1.4049372 ] converges to x_(k+1) = [0.1776073 1.4088203]\n",
      "There is still 0.2470517400079135 left for approximations to converge \n",
      "\n",
      "x_k = [0.1776073 1.4088203] converges to x_(k+1) = [0.17758334 1.40862696]\n",
      "There is still 0.01139921425689792 left for approximations to converge \n",
      "\n",
      "x_k = [0.17758334 1.40862696] converges to x_(k+1) = [0.17758343 1.40862772]\n",
      "\n",
      "Final approximation of the minima is [0.17758343 1.40862772].\n",
      "Optimization process converged in 7 steps\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(n):\n",
    "    x_start = np.random.randint(-100, 100, num_of_args[i])\n",
    "    print('\\nMinimizing function {0} with the starting point {1}\\n'.format(functions[i].__name__, x_start))\n",
    "    BFGS(functions[i], gradients[i], x_start, 0.001, max_iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
